{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jovianlin.io/embeddings-in-keras/\n",
    "# 참고사이트\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Bidirectional, LSTM\n",
    "docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!',\n",
    "        'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']\n",
    "\n",
    "labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3], [3, 9], [9, 4], [7, 9], [7], [5], [6, 4], [7, 3], [6, 9], [3, 8, 3, 2]]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "own_embedding_vocab_size = 10\n",
    "encoded_docs_oe = [one_hot(d, own_embedding_vocab_size) for d in docs]\n",
    "print(encoded_docs_oe)\n",
    "print(len(encoded_docs_oe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 0 0 0]\n",
      " [3 9 0 0 0]\n",
      " [9 4 0 0 0]\n",
      " [7 9 0 0 0]\n",
      " [7 0 0 0 0]\n",
      " [5 0 0 0 0]\n",
      " [6 4 0 0 0]\n",
      " [7 3 0 0 0]\n",
      " [6 9 0 0 0]\n",
      " [3 8 3 2 0]]\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 5\n",
    "padded_decs_oe = pad_sequences(encoded_docs_oe, maxlen=maxlen, padding='post')\n",
    "print(padded_decs_oe)\n",
    "print(padded_decs_oe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=own_embedding_vocab_size, output_dim=32, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32,activation='relu',recurrent_dropout=0.1,return_sequences)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 32)             320       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 17,025\n",
      "Trainable params: 17,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])  # Compile the model\n",
    "print(model.summary())  # Summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.153 Accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_decs_oe, labels, epochs=50, verbose=0)  # Fit the model\n",
    "loss, accuracy = model.evaluate(padded_decs_oe, labels, verbose=0)  # Evaluate the model\n",
    "print('loss : %0.3f'%loss ,'Accuracy: %0.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 9, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_decs_oe[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Good work\"\n",
    "word_docs_oe = [one_hot(word, own_embedding_vocab_size)]\n",
    "word_oe = pad_sequences(word_docs_oe, maxlen=maxlen, padding='post')\n",
    "pred = model.predict(word_oe)\n",
    "np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = padded_decs_oe\n",
    "a = model.predict(s)\n",
    "np.round(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 9, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25155085],\n",
       "       [0.88942003],\n",
       "       [0.9993516 ],\n",
       "       [0.9988058 ],\n",
       "       [0.76390016],\n",
       "       [0.39815846],\n",
       "       [0.0150933 ],\n",
       "       [0.34821892],\n",
       "       [0.0957509 ],\n",
       "       [0.00982311]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = padded_decs_oe\n",
    "a = model.predict(s)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "- 분류 : 감정분류 ( 긍 부 정 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 125   68    2    2   15  349  165    2   98    5    4  228    9   43\n",
      "    2 1157   15  299  120    5  120  174   11  220  175  136   50    9\n",
      "    2  228    2    5    2  656  245    2    5    4    2  131  152  491\n",
      "   18    2   32    2 1212   14    9    6  371   78   22  625   64 1382\n",
      "    9    8  168  145   23    4 1690   15   16    4 1355    5   28    6\n",
      "   52  154  462   33   89   78  285   16  145   95]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM, Embedding,Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "vocab_size = 2000\n",
    "maxlen=80\n",
    "\n",
    "(x_train, y_train), (x_test,y_test) = imdb.load_data(num_words = vocab_size)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen,padding='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen,padding='post')\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=128,input_length=maxlen))\n",
    "model.add(LSTM(128,dropout = 0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['acc'])\n",
    "\n",
    "che = 'keras_model1'\n",
    "point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 387,713\n",
      "Trainable params: 387,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4669 - acc: 0.7756\n",
      "Epoch 00001: val_loss improved from inf to 0.38493, saving model to keras_model1\n",
      "INFO:tensorflow:Assets written to: keras_model1\\assets\n",
      "782/782 [==============================] - 88s 113ms/step - loss: 0.4669 - acc: 0.7756 - val_loss: 0.3849 - val_acc: 0.8252\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3666 - acc: 0.8387\n",
      "Epoch 00002: val_loss improved from 0.38493 to 0.38231, saving model to keras_model1\n",
      "INFO:tensorflow:Assets written to: keras_model1\\assets\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.3666 - acc: 0.8387 - val_loss: 0.3823 - val_acc: 0.8325\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3251 - acc: 0.8600\n",
      "Epoch 00003: val_loss improved from 0.38231 to 0.35752, saving model to keras_model1\n",
      "INFO:tensorflow:Assets written to: keras_model1\\assets\n",
      "782/782 [==============================] - 82s 105ms/step - loss: 0.3251 - acc: 0.8600 - val_loss: 0.3575 - val_acc: 0.8408\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2956 - acc: 0.8740\n",
      "Epoch 00004: val_loss did not improve from 0.35752\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.2956 - acc: 0.8740 - val_loss: 0.3620 - val_acc: 0.8387\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2638 - acc: 0.8906\n",
      "Epoch 00005: val_loss did not improve from 0.35752\n",
      "782/782 [==============================] - 82s 105ms/step - loss: 0.2638 - acc: 0.8906 - val_loss: 0.3860 - val_acc: 0.8324\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2417 - acc: 0.9013\n",
      "Epoch 00006: val_loss did not improve from 0.35752\n",
      "782/782 [==============================] - 92s 118ms/step - loss: 0.2417 - acc: 0.9013 - val_loss: 0.4206 - val_acc: 0.8320\n",
      "Epoch 7/10\n",
      "554/782 [====================>.........] - ETA: 22s - loss: 0.2134 - acc: 0.9156"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=32, epochs=10,validation_data = (x_test, y_test),callbacks=[point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('keras_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] 0\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train[1])\n",
    "print(np.round(pred),y_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tsukino'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = imdb.get_word_index(path='imdb_word_index.json')\n",
    "imdb_key=dictionary.keys()\n",
    "list(imdb_key)[1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
